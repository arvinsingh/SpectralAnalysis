{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.use('TKAGG')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "# from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import cluster\n",
    "import sys\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "path_to_mitie_lib = '/opt/anaconda/anaconda3/lib/python3.6/site-packages/mitie'\n",
    "path_to_ner_model = '/home/somnus/MITIE-master/MITIE-models/english/ner_model.dat'\n",
    "\n",
    "sys.path.append(path_to_mitie_lib)\n",
    "\n",
    "from mitie import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle, islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of articles to process\n",
    "N = 50\n",
    "\n",
    "# in memory stores for the topics, titles and contents of the news stories\n",
    "# topics_array = []\n",
    "titles_array = []\n",
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, N):\n",
    "\n",
    "    # get the contents of the article\n",
    "    # art = '/home/somnus/data_sets/articles/article-' + str(i) + '.txt'\n",
    "    x = str(i)\n",
    "    with open('/home/somnus/data_sets/art_copy/article-' + x + '.txt', 'r') as myfile:\n",
    "        d1 = myfile.read()\n",
    "        d1 = d1.lower()\n",
    "        corpus.append(d1)\n",
    "\n",
    "    # get the title of the article\n",
    "    # titl = '/home/somnus/data_sets/articles/title-' + str(i) + '.txt'\n",
    "    with open('/home/somnus/data_sets/art_copy/title-' + x + '.txt', 'r') as myfile:\n",
    "        ti1 = myfile.read()\n",
    "        ti1 = ti1.lower()\n",
    "        titles_array.append(ti1)\n",
    "\n",
    "    # get the original topic of the article\n",
    "    # top = '/home/somnus/data_sets/articles/topic-' + str(i) + '.txt'\n",
    "    '''with open('topic-' + str(i) + '.txt', 'r') as myfile:\n",
    "        to1 = myfile.read().replace('\\n', '')\n",
    "        to1 = to1.lower()\n",
    "        topics_array.append(to1)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity subset array\n",
    "entity_text_array = []\n",
    "\n",
    "ner = named_entity_extractor(path_to_ner_model)\n",
    "\n",
    "for i in range(0, N):\n",
    "\n",
    "    # Load the article contents text file and convert it into a list of words.\n",
    "    tokens = tokenize(corpus[i])\n",
    "    # tokens = tokenize(load_entire_file('/home/somnus/data_sets/articles_1/article-' + str(i) + '.txt'))\n",
    "\n",
    "\n",
    "    # extract all entities known to the ner model mentioned in this article\n",
    "    entities = ner.extract_entities(tokens)\n",
    "\n",
    "    # extract the actual entity words and append to the array\n",
    "    for e in entities:\n",
    "        range_array = e[0]\n",
    "        tag = e[1]\n",
    "        score = e[2]\n",
    "        score_text = \"{:0.3f}\".format(score)\n",
    "        # entity_text = \" \".join(tokens[j] for j in range_array)\n",
    "        entity_text = \" \".join(tokens[i].decode() for i in range_array)\n",
    "        entity_text_array.append(entity_text.lower())\n",
    "        \n",
    "# remove duplicate entities detected\n",
    "entity_text_array = np.unique(entity_text_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(sublinear_tf=True,\n",
    "                       max_df=0.5,\n",
    "                       analyzer='word',\n",
    "                       stop_words='english',\n",
    "                       vocabulary=entity_text_array)\n",
    "\n",
    "# corpus_tf_idf = vect.fit_transform(corpus)\n",
    "\n",
    "X = vect.fit_transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/pca.py:425: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=2).fit(X)\n",
    "data2D = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpectralClustering(affinity='nearest_neighbors', assign_labels='kmeans',\n",
       "          coef0=1, degree=3, eigen_solver='arpack', eigen_tol=0.0,\n",
       "          gamma=1.0, kernel_params=None, n_clusters=5, n_init=10, n_jobs=1,\n",
       "          n_neighbors=10, random_state=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change n_clusters to equal the number of clusters desired\n",
    "n_clusters = 5\n",
    "# n_components = n_clusters\n",
    "\n",
    "# spectral clustering\n",
    "spectral = cluster.SpectralClustering(n_clusters=n_clusters,\n",
    "                                      eigen_solver='arpack',\n",
    "                                      affinity=\"nearest_neighbors\",\n",
    "                                      n_neighbors=10)\n",
    "\n",
    "spectral.fit(data2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S.no Cluster Number Title of Article\n"
     ]
    }
   ],
   "source": [
    "# OUTPUT\n",
    "# article_number, topic, spectral_clustering_cluster_number, article_title\n",
    "cluster_assignments = np.int()\n",
    "\n",
    "\n",
    "if hasattr(spectral, 'labels_'):\n",
    "    cluster_assignments = spectral.labels_.astype(np.int)\n",
    "\n",
    "print(\"S.no\", \"Cluster Number\", \"Title of Article\")\n",
    "\n",
    "data = list()\n",
    "\n",
    "for i in range(0, len(cluster_assignments)):\n",
    "\n",
    "    data.append([int(i), int(cluster_assignments[i]), str(titles_array[i])])\n",
    "    # print(i, topics_array[i], cluster_assignments[i], titles_array[i])\n",
    "    # print(data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                    '#f781bf', '#a65628', '#984ea3',\n",
    "                                    '#999999', '#e41a1c', '#dede00']),\n",
    "                                    int(max(cluster_assignments) + 1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Spectral Clustering')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [data[i][0] for i in range(0, 50)]\n",
    "y = [data[j][1] for j in range(0, 50)]\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(x, y, s=5, color=colors[cluster_assignments])\n",
    "ax.set_ylim(-1, 5)\n",
    "ax.set_yticklabels(['0', 'top_1', 'top_2',\n",
    "               'top_3', 'top_4',\n",
    "               'top_5'])\n",
    "ax.set_title(\"Spectral Clustering\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots()\n",
    "ax1.scatter(data2D[:,0], data2D[:,1], s=10, color=colors[cluster_assignments])\n",
    "fig1.savefig('/home/somnus/data_sets/clusters/spectral_science_' + str(n_clusters), format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
