{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk, string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of articles to process\n",
    "N = 30\n",
    "\n",
    "# in memory stores for the topics, titles and contents of the news stories\n",
    "# topics_array = []\n",
    "titles_array = []\n",
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, N):\n",
    "\n",
    "    # get the contents of the article\n",
    "    # art = '/home/somnus/data_sets/articles/article-' + str(i) + '.txt'\n",
    "    x = str(i)\n",
    "    with open('/home/sourabh/AP/P/nyt/science/article-' + x + '.txt', 'r') as myfile:\n",
    "        d1 = myfile.read().replace('\\n', '')\n",
    "\n",
    "        d1 = d1.lower()\n",
    "        corpus.append(d1)\n",
    "\n",
    "    # get the title of the article\n",
    "    # titl = '/home/somnus/data_sets/articles/title-' + str(i) + '.txt'\n",
    "    with open('/home/sourabh/AP/P/nyt/science/title-' + x + '.txt', 'r') as myfile:\n",
    "        ti1 = myfile.read().replace('\\n', '')\n",
    "        ti1 = ti1.lower()\n",
    "        titles_array.append(ti1)\n",
    "\n",
    "    # get the original topic of the article\n",
    "    # top = '/home/somnus/data_sets/articles/topic-' + str(i) + '.txt'\n",
    "    '''with open('topic-' + str(i) + '.txt', 'r') as myfile:\n",
    "        to1 = myfile.read().replace('\\n', '')\n",
    "        to1 = to1.lower()\n",
    "        topics_array.append(to1)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean texts\n",
    "def simple_cleaner(mess):\n",
    "\n",
    "    # Check for punctuations\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    # Remove any stopwords\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAG of Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_transformer = CountVectorizer(analyzer=simple_cleaner).fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photohoesung lee, a korean economist who was a vice chairman of the intergovernmental panel on climate change, has been elected chairman, defeating european and american candidates with far more prominent public profiles. lee is a professor at korea university in seoul, specializing in the economics of climate change, energy and sustainable development.he prevailed in a field of six candidates after a runoff vote with jean-pascale van ypersele, a belgian physicist with 20 years of i.p.c.c. experience. a news release has more information.in some ways, the choice is apt, given the importance of asia in coming years, both as the dominant source of growth in greenhouse gas emissions (india in particular) and as a rising force in energy research and innovation (south korea, china and japan).but climate science is still dominated by western labs and government agencies.and, at least in his recent video interview with carbon brief, it seems lee, like many economists who came of age at the peak of the traditional environmental movement, has a very locked-in view that raising the cost of polluting is the critical way to shift global economies away from cheap fossil fuels.just price that externality and all will be well. that’s true in a rational system. but the real world hasn’t proved very rational in this arena, leading other analysts in this arena to focus as much on spurring innovation to make clean energy cheap as they do on finding ways to regulate or tax polluters. (see the deep decarbonization pathways reports, for instance.)listen to him here or read the transcript below and then the full interview.here’s lee on carbon pricing:climate change is a typical example of externalities and the way to correct the externality problem is to have a price on certain activities that cause those externalities. in our case, that is a price on carbon emissions – what you may call a carbon tax. now, i think if you ask me to choose the most important work in climate change issues, then i’ll choose carbon price. that’s because it is the driver to put us into the right track. i would like to pursue, as much as possible, to increase our knowledge of carbon price and future emissions, and our knowledge on reducing the institutional barriers to adopting a carbon price system.if the barriers were only institutional, i’d cheer him on. india’s barriers are existential for politicians in a country where 300 million people can’t turn on a light. un-inventing suburbia is about infrastructure and culture as much as institutions.read the full interview or watch more video excerpts here.i wish him well and hope the panel finds ways to remain relevant as the energy and climate landscape continues to change.and read my earlier post on the i.p.c.c.’s continuing issues with connecting with the public and media: “a top task for the new chair of the u.n. climate panel – a communication reboot.“\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "print(corpus[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 77)\t1\n",
      "  (0, 120)\t1\n",
      "  (0, 265)\t1\n",
      "  (0, 311)\t1\n",
      "  (0, 357)\t1\n",
      "  (0, 361)\t1\n",
      "  (0, 470)\t1\n",
      "  (0, 490)\t1\n",
      "  (0, 596)\t1\n",
      "  (0, 612)\t2\n",
      "  (0, 661)\t1\n",
      "  (0, 664)\t1\n",
      "  (0, 769)\t1\n",
      "  (0, 832)\t3\n",
      "  (0, 893)\t1\n",
      "  (0, 1119)\t1\n",
      "  (0, 1204)\t1\n",
      "  (0, 1211)\t1\n",
      "  (0, 1227)\t2\n",
      "  (0, 1235)\t1\n",
      "  (0, 1257)\t7\n",
      "  (0, 1284)\t1\n",
      "  (0, 1302)\t1\n",
      "  (0, 1334)\t1\n",
      "  (0, 1342)\t1\n",
      "  :\t:\n",
      "  (0, 7986)\t1\n",
      "  (0, 8015)\t1\n",
      "  (0, 8038)\t1\n",
      "  (0, 8049)\t1\n",
      "  (0, 8104)\t1\n",
      "  (0, 8118)\t1\n",
      "  (0, 8169)\t1\n",
      "  (0, 8206)\t1\n",
      "  (0, 8247)\t1\n",
      "  (0, 8251)\t2\n",
      "  (0, 8255)\t1\n",
      "  (0, 8298)\t1\n",
      "  (0, 8361)\t1\n",
      "  (0, 8373)\t2\n",
      "  (0, 8374)\t3\n",
      "  (0, 8408)\t2\n",
      "  (0, 8423)\t1\n",
      "  (0, 8489)\t1\n",
      "  (0, 8518)\t1\n",
      "  (0, 8531)\t1\n",
      "  (0, 8549)\t1\n",
      "  (0, 8592)\t2\n",
      "  (0, 8636)\t1\n",
      "  (0, 8659)\t2\n",
      "  (0, 8760)\t1\n",
      "(1, 8936)\n"
     ]
    }
   ],
   "source": [
    "bow4 = bow_transformer.transform([corpus[3]])\n",
    "print(bow4)\n",
    "print(bow4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carbon\n",
      "candidates\n"
     ]
    }
   ],
   "source": [
    "print(bow_transformer.get_feature_names()[1257])\n",
    "print(bow_transformer.get_feature_names()[1227])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus has 8936 words\n"
     ]
    }
   ],
   "source": [
    "print('The corpus has {} words'.format(len(bow_transformer.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_bow = bow_transformer.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "## This is without ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8760)\t0.03791068209619584\n",
      "  (0, 8659)\t0.0928750291390883\n",
      "  (0, 8636)\t0.056998956929512734\n",
      "  (0, 8592)\t0.04539231541384443\n",
      "  (0, 8549)\t0.017341929007246848\n",
      "  (0, 8531)\t0.02629799945725862\n",
      "  (0, 8518)\t0.023519976741356426\n",
      "  (0, 8489)\t0.056998956929512734\n",
      "  (0, 8423)\t0.04643751456954415\n",
      "  (0, 8408)\t0.05469847947245451\n",
      "  (0, 8374)\t0.12911246865401033\n",
      "  (0, 8373)\t0.04703995348271285\n",
      "  (0, 8361)\t0.04643751456954415\n",
      "  (0, 8298)\t0.05082090919540316\n",
      "  (0, 8255)\t0.018514579813192232\n",
      "  (0, 8251)\t0.08051893367086915\n",
      "  (0, 8247)\t0.04643751456954415\n",
      "  (0, 8206)\t0.056998956929512734\n",
      "  (0, 8169)\t0.02847841849979578\n",
      "  (0, 8118)\t0.023519976741356426\n",
      "  (0, 8104)\t0.056998956929512734\n",
      "  (0, 8049)\t0.056998956929512734\n",
      "  (0, 8038)\t0.04643751456954415\n",
      "  (0, 8015)\t0.03587607220957556\n",
      "  (0, 7986)\t0.03587607220957556\n",
      "  :\t:\n",
      "  (0, 1342)\t0.056998956929512734\n",
      "  (0, 1334)\t0.031023811650990683\n",
      "  (0, 1302)\t0.03587607220957556\n",
      "  (0, 1284)\t0.03587607220957556\n",
      "  (0, 1257)\t0.32506260198680903\n",
      "  (0, 1235)\t0.03791068209619584\n",
      "  (0, 1227)\t0.0928750291390883\n",
      "  (0, 1211)\t0.027349239736227254\n",
      "  (0, 1204)\t0.03587607220957556\n",
      "  (0, 1119)\t0.056998956929512734\n",
      "  (0, 893)\t0.056998956929512734\n",
      "  (0, 832)\t0.1709968707885382\n",
      "  (0, 769)\t0.032476047191368186\n",
      "  (0, 664)\t0.03791068209619584\n",
      "  (0, 661)\t0.04643751456954415\n",
      "  (0, 612)\t0.11399791385902547\n",
      "  (0, 596)\t0.056998956929512734\n",
      "  (0, 490)\t0.04303748955133677\n",
      "  (0, 470)\t0.027349239736227254\n",
      "  (0, 361)\t0.056998956929512734\n",
      "  (0, 357)\t0.040259466835434576\n",
      "  (0, 311)\t0.056998956929512734\n",
      "  (0, 265)\t0.05082090919540316\n",
      "  (0, 120)\t0.05082090919540316\n",
      "  (0, 77)\t0.03587607220957556\n"
     ]
    }
   ],
   "source": [
    "# tf_idf for third article\n",
    "tfidf_transformer = TfidfTransformer().fit(articles_bow)\n",
    "tfidf4 = tfidf_transformer.transform(bow4)\n",
    "print(tfidf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 8936)\n"
     ]
    }
   ],
   "source": [
    "# tf_idf for all 30\n",
    "articles_tfidf = tfidf_transformer.transform(articles_bow)\n",
    "print(articles_tfidf.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
